# Procedures d'exploitation

## Vue d'ensemble

Ce document decrit les procedures operationnelles pour l'exploitation
quotidienne, hebdomadaire et mensuelle de l'infrastructure SaaS HA.

---

## Operations quotidiennes

### 1. Verification matinale (5 min)

```bash
# 1. Etat du cluster
kubectl get nodes -o wide
kubectl get pods --all-namespaces | grep -v Running | grep -v Completed

# 2. Etat des bases de donnees
kubectl exec -n production postgresql-0 -- patronictl list
kubectl exec -n production redis-0 -- redis-cli INFO replication | grep role

# 3. Verification rapide des alertes
kubectl exec -n monitoring alertmanager-0 -- \
  curl -s http://localhost:9093/api/v2/alerts | python3 -c "
import json, sys
alerts = json.load(sys.stdin)
active = [a for a in alerts if a.get('status', {}).get('state') == 'active']
print(f'Alertes actives: {len(active)}')
for a in active:
    labels = a.get('labels', {})
    print(f'  [{labels.get(\"severity\",\"?\")}] {labels.get(\"alertname\",\"?\")}')
"

# 4. Espace disque
kubectl exec -n production postgresql-0 -- df -h /var/lib/postgresql/data
kubectl exec -n monitoring prometheus-0 -- df -h /prometheus

# 5. Verifier ArgoCD
kubectl get applications -n argocd
```

### 2. Verification des logs d'erreur

```bash
# Errors dans les 30 dernieres minutes
kubectl logs -n production -l app=backend --tail=50 --since=30m | grep -i error
kubectl logs -n production -l app=frontend --tail=50 --since=30m | grep -i error
kubectl logs -n ingress -l app=traefik --tail=50 --since=30m | grep -i "500\|502\|503"
```

### 3. Metriques cles a surveiller

| Metrique | Seuil normal | Alerte |
|----------|-------------|--------|
| CPU nodes | < 70% | > 85% |
| Memoire nodes | < 80% | > 90% |
| Disque PG | < 70% | > 85% |
| Disque Prometheus | < 80% | > 90% |
| Replication lag PG | < 1s | > 10s |
| Redis memory | < maxmemory 80% | > 90% |
| HTTP error rate | < 1% | > 5% |
| Latence p95 | < 300ms | > 500ms |
| Certificats TLS | > 30 jours | < 7 jours |

---

## Operations hebdomadaires

### 1. Revue des metriques (lundi matin, 15 min)

```bash
# SLA de la semaine
./scripts/tests/validate-sla.sh --period 7d

# Tendances ressources
kubectl top nodes
kubectl top pods --all-namespaces --sort-by=cpu | head -20
kubectl top pods --all-namespaces --sort-by=memory | head -20
```

### 2. Verification des backups (lundi, 10 min)

```bash
# Lister les backups PostgreSQL
kubectl exec -n production postgresql-0 -- wal-g backup-list

# Verifier le dernier backup
kubectl exec -n production postgresql-0 -- \
  wal-g backup-list --json | python3 -c "
import json, sys
from datetime import datetime, timezone
data = json.load(sys.stdin)
if data:
    last = data[-1]
    print(f'Dernier backup: {last[\"backup_name\"]}')
    print(f'Date: {last[\"time\"]}')
    print(f'Total backups: {len(data)}')
"

# Verifier la retention (garder minimum 7 jours)
kubectl exec -n production postgresql-0 -- \
  wal-g delete --confirm retain FULL 7 --dry-run
```

### 3. Revue de securite (mercredi, 15 min)

```bash
# Scan securite rapide
./scripts/tests/test-security-scan.sh

# Verifier les certificats
kubectl get certificates --all-namespaces
kubectl describe certificate -n ingress | grep -A2 "Not After"

# Verifier les images vulnerables (si Trivy disponible)
trivy image --severity HIGH,CRITICAL backend:latest
trivy image --severity HIGH,CRITICAL frontend:latest
```

### 4. Test de failover (vendredi, optionnel, 20 min)

```bash
# Test failover PostgreSQL (dry-run d'abord)
./scripts/tests/test-failover-postgresql.sh --dry-run
# Puis en reel si necessaire
# ./scripts/tests/test-failover-postgresql.sh

# Test failover Redis
./scripts/tests/test-failover-redis.sh --dry-run
```

### 5. Nettoyage

```bash
# Nettoyer les pods termines
kubectl delete pods --field-selector=status.phase=Succeeded --all-namespaces
kubectl delete pods --field-selector=status.phase=Failed --all-namespaces

# Nettoyer les images Docker non utilisees (sur chaque node)
ssh root@node1 "crictl rmi --prune"
ssh root@node2 "crictl rmi --prune"

# Verifier les PV inutilises
kubectl get pv | grep Released
```

---

## Operations mensuelles

### 1. Revue complete (1er lundi du mois, 30 min)

```bash
# Rapport SLA mensuel
./scripts/tests/validate-sla.sh --period 30d

# Suite de tests complete
./scripts/tests/run-all-tests.sh --report

# Revue des alertes du mois
kubectl exec -n monitoring alertmanager-0 -- \
  curl -s "http://localhost:9093/api/v2/alerts?filter=state%3Dresolved" | python3 -c "
import json, sys
alerts = json.load(sys.stdin)
print(f'Alertes resolues ce mois: {len(alerts)}')
"
```

### 2. Mise a jour de securite

```bash
# 1. Verifier les mises a jour disponibles (sur chaque node)
ssh root@node1 "apt update && apt list --upgradable"
ssh root@node2 "apt update && apt list --upgradable"

# 2. Appliquer les mises a jour de securite (node par node)
# IMPORTANT: Drainer le node avant la mise a jour
kubectl drain node-2 --ignore-daemonsets --delete-emptydir-data
ssh root@node2 "apt upgrade -y && reboot"
# Attendre le redemarrage
kubectl uncordon node-2
kubectl get nodes

# 3. Puis pour node-1
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data
ssh root@node1 "apt upgrade -y && reboot"
kubectl uncordon node-1
```

### 3. Rotation des secrets

```bash
# 1. Generer de nouveaux secrets
openssl rand -base64 32  # Nouveau JWT_SECRET
openssl rand -base64 32  # Nouveau mot de passe DB

# 2. Creer le SealedSecret
kubectl create secret generic backend-secrets \
  --from-literal=JWT_SECRET="nouveau_secret" \
  --dry-run=client -o yaml | kubeseal --format yaml \
  > kubernetes/apps/backend/backend-sealed-secret.yaml

# 3. Committer et deployer via GitOps
git add kubernetes/apps/backend/backend-sealed-secret.yaml
git commit -m "rotate: monthly secret rotation"
git push

# 4. Verifier le deploiement
argocd app sync backend
kubectl rollout status deployment/backend -n production
```

### 4. Nettoyage des backups

```bash
# Appliquer la politique de retention WAL-G
kubectl exec -n production postgresql-0 -- \
  wal-g delete --confirm retain FULL 30

# Verifier l'espace S3
kubectl exec -n production postgresql-0 -- \
  wal-g st ls --config /etc/wal-g/config.yaml
```

### 5. Revue des capacites

```bash
# Verifier les tendances de croissance
# Via Grafana : Dashboard Infrastructure > Storage trends

# Verifier les limites HPA
kubectl get hpa --all-namespaces

# Verifier les quotas de ressources
kubectl describe resourcequota -n production
```

---

## Operations trimestrielles

### 1. Test PRA/PCA complet (Plan de Reprise/Continuite d'Activite)

```bash
# 1. Test de restauration complete depuis backup
./scripts/tests/test-backup-restore.sh

# 2. Test de perte d'un node complet
# Simuler la perte de Node 2
hcloud server shutdown node-2
# Verifier que les services restent disponibles
kubectl get pods --all-namespaces -o wide
curl -s -o /dev/null -w "%{http_code}" https://app.saas.local
# Remettre Node 2
hcloud server poweron node-2
kubectl get nodes

# 3. Test de reconstruction du cluster depuis Git
# Documentation : docs/architecture/architecture.md (section Disaster Recovery)
```

### 2. Audit de securite approfondi

```bash
# Scan complet de toutes les images
./scripts/tests/test-security-scan.sh --full

# Audit RBAC
kubectl auth can-i --list --as=system:serviceaccount:production:default

# Verifier les NetworkPolicies
kubectl get networkpolicies --all-namespaces

# Test de penetration (outils externes)
# nikto, OWASP ZAP, nuclei...
```

### 3. Mise a jour K3s

```bash
# 1. Verifier la version actuelle
kubectl version --short

# 2. Mettre a jour le worker (Node 2) d'abord
kubectl drain node-2 --ignore-daemonsets --delete-emptydir-data
ssh root@node2 "curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=stable sh -"
kubectl uncordon node-2

# 3. Puis le server (Node 1)
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data
ssh root@node1 "curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=stable sh -"
kubectl uncordon node-1

# 4. Verifier
kubectl get nodes
kubectl get pods --all-namespaces
```

---

## Procedures d'urgence

### Perte de Node 1 (Control Plane)

```
Impact: Control plane down, services sur Node 1 indisponibles
RTO: Attendre reboot auto Hetzner (~5min) ou reboot manuel

1. Verifier via Hetzner Console/API
   hcloud server describe node-1

2. Tenter un reboot
   hcloud server reboot node-1

3. Si le serveur ne repond pas
   - Contacter le support Hetzner (24/7)
   - Envisager un rebuild depuis Terraform + restore backup

4. Post-recovery
   kubectl get nodes
   kubectl get pods --all-namespaces
   kubectl exec -n production postgresql-0 -- patronictl list
```

### Perte de Node 2 (Worker)

```
Impact: Replicas HA perdus, mais service maintenu via Node 1
RTO: < 5 min (pods rescheduled sur Node 1)

1. Les pods sont automatiquement reschedulÃ©s sur Node 1
2. Verifier la charge sur Node 1
   kubectl top node node-1
3. Redemarrer Node 2 quand possible
   hcloud server reboot node-2
4. Verifier la redistribution des pods
   kubectl get pods --all-namespaces -o wide
```

### Base de donnees corrompue

```
1. Arreter les ecritures
   kubectl scale deployment backend -n production --replicas=0

2. Evaluer la corruption
   kubectl exec -n production postgresql-0 -- pg_isready
   kubectl exec -n production postgresql-0 -- psql -U postgres -c "SELECT pg_is_in_recovery();"

3. Restaurer depuis WAL-G
   kubectl exec -n production postgresql-0 -- wal-g backup-fetch /tmp/restore LATEST

4. Ou Point-in-Time Recovery
   # Voir docs/runbooks/postgresql.md

5. Redemarrer le backend
   kubectl scale deployment backend -n production --replicas=3
```

### Certificat TLS expire

```
1. Verifier le certificat
   kubectl get certificates -n ingress
   kubectl describe certificate <name> -n ingress

2. Forcer le renouvellement
   kubectl delete certificate <name> -n ingress
   # cert-manager renouvelle automatiquement

3. Verifier
   kubectl get certificates -n ingress
   kubectl describe order -n ingress

4. Si cert-manager echoue
   kubectl logs -n cert-manager -l app=cert-manager
   # Verifier les challenges HTTP-01
   kubectl get challenges -n ingress
```

---

## Checklist operationnelle

### Quotidien
- [ ] Verifier l'etat des nodes (`kubectl get nodes`)
- [ ] Verifier les pods non-Running
- [ ] Verifier les alertes actives
- [ ] Verifier les logs d'erreur (backend, frontend, traefik)

### Hebdomadaire
- [ ] Revue des metriques SLA
- [ ] Verification des backups WAL-G
- [ ] Scan de securite rapide
- [ ] Nettoyage des pods termines

### Mensuel
- [ ] Rapport SLA complet
- [ ] Suite de tests complete
- [ ] Mise a jour de securite (OS)
- [ ] Rotation des secrets
- [ ] Nettoyage des backups anciens
- [ ] Revue des capacites (CPU, memoire, disque)

### Trimestriel
- [ ] Test PRA/PCA complet
- [ ] Audit de securite approfondi
- [ ] Mise a jour K3s
- [ ] Revue d'architecture
